
<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tete Xiao</title>
  
  <meta name="author" content="Tete Xiao">
  <meta name="descripction" content="Tete Xiao is a Ph.D. student at BAIR, UC Berkeley.">
  <meta name="keywords" content="Tete Xiao, UC Berkeley, BAIR, Ph.D. student, Computer Science">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">

  <script src="scripts/functions.js"></script>
</head>

<body data-gr-c-s-loaded="true">
<table width="840" border="0" align="center" cellspacing="0" cellpadding="20"><tbody><tr><td>

<!-- title -->

<p align="center">
    <pageheading>Tete Xiao</pageheading><br>
    <b>Email</b> : <font id="email" style="display:inline;">jasonhsiao97 [AT] gmail [DOT] com</font>
</p>

<!-- avatar and bio -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody><tr>
    <td width="32%" valign="center">
        <img src="images/tetexiao.jpg" width="100%" style="border-radius:20px">
        <p align="center">
        <!-- TODO:: update cv path  <02-10-19, YL> -->
        <!-- TODO:: add linked in  <06-10-19, YL> -->
        | <a href="pdfs/resume.pdf">CV</a> | <a href="https://scholar.google.com/citations?user=U4RqBdAAAAAJ&hl=en">Google Scholar</a> | <a href="https://github.com/Tete-Xiao">Github</a> |
        </p>
    </td>

    <td width="68%" valign="center" align="justify">
        <p> I'm a 3rd-year Ph.D. student at the Berkeley Artificial Intelligence Research (<strong>BAIR</strong>) Lab at <strong>University of California, Berkeley</strong>, advised by <a href='https://people.eecs.berkeley.edu/~trevor/'>Prof. Trevor Darrell</a>. My research interests lie in the field of <strong>Computer Vision</strong> and <strong>Machine Learning</strong>, particularly in enabling intelligent agents perceive, comprehend and reason with less direct supervision. I'm also affiliated with Facebook AI Research (<strong>FAIR</strong>) in collaboration with <a href='https://pdollar.github.io/'>Piotr Dollár</a> and <a href='https://www.rossgirshick.info/'>Ross Girshick</a>. Prior to UC Berkeley, I collaborated with <a href='http://bzhou.ie.cuhk.edu.hk/'>Bolei Zhou</a> on a series of works, and was mentored by <a href='https://yuningjiang.github.io/'>Yuning Jiang</a> and supervised by <a href='http://www.jiansun.org/'>Jian Sun</a>.</p>
        <p> I graduated from <strong>Peking University (PKU)</strong> in 2019, the most progressive university in China, summa cum laude with a Bachelor's degree. I started computer programming in primary school and had been participating in programming contests throughout high school and college. I received China National Scholarship in 2016 and was selected as a <a href='https://snapresearchfs.splashthat.com/'>2019 Snap Research Scholar</a> for my research and curriculum.</p>
        <p> I have a great fondness for arts besides my academic work. I'm deeply passionate about musical works, especially those in classical music (both instrumental & vocal) and jazz. I'm also into laws & public policies. I'm a political activist in both the U.S. and my home country. </p>
    </td>
</tr></tbody></table>

<!-- news -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tbody><tr><td>
        <sectionheading>&nbsp;&nbsp;News</sectionheading>
            <ul>
                <li> <b>[July 2021]</b> Two papers accepted to ICCV21! </li>
                <li> <b>[Jan. 2021]</b> Two papers (including one selected for oral presentation) accepted to ICLR21! </li>
                <li> <b>[Mar. 2020]</b> Our work on compositional action recognition is accepted to <a href='http://cvpr2020.thecvf.com/'>CVPR20</a>! Check out the <a href='https://joaanna.github.io/something_else/'>project page</a> with the new dataset "Something-else"! </li>
                <li> <b>[July 2019]</b> One paper on explainable human-object interaction is accepted to <a href='http://iccv2019.thecvf.com/'>ICCV19</a>! </li>
                <li> <b>[May 2019]</b> I will join the wonderful Berkeley Artificial Intelligence Research (BAIR) Lab as a Ph.D. student at the lovely UC Berkeley in August 2019. <strong>Go Bears!</strong> </li>
                <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
                <div id="old_news" style="display: none;">
                <li> <b>[Nov. 2018]</b> <a href='http://sceneparsing.csail.mit.edu/'>ADE20K</a> accepted to IJCV! We included a variety of interesting applications plus the study of synchronized batch norm for semantic segmentation. Check out the <a href='https://arxiv.org/pdf/1608.05442.pdf'>paper</a> for more details! </li>
                <li> <b>[July 2018]</b> Two papers (including one oral) accepted to <a href='https://eccv2018.org/'>ECCV18</a>! (Code is released!) </li>
                <li> <b>[May 2018]</b> Our work on contrastive samples in vision and language learning accepted to <a href='https://coling2018.org/'>COLING18</a>! (<a href='https://arxiv.org/abs/1806.10348'>Paper</a> and <a href='https://github.com/ExplorerFreda/VSE-C'>codes</a> are released.) </li>
                <li> <b>[Apr. 2018]</b> A PyTorch implementation of scene parsing networks trained on ADE20K with SOTA performance is released in conjunction with MIT CSAIL. Check out our <a href='https://github.com/CSAILVision/semantic-segmentation-pytorch'>code</a>, it's popular! </li>
                <li> <b>[Feb. 2018]</b> Two papers accepted to <a href='http://cvpr2018.thecvf.com/'>CVPR18</a>! </li>
                <li> <b>[Oct. 2017]</b> As a team member of Megvii (Face++), we won the premier challenge of object detection - <a href='https://places-coco2017.github.io/'>COCO and Places Challenges 2017</a>: the 1st places of <a href='http://cocodataset.org/#detections-leaderboard'>COCO Detection</a>, <a href='http://cocodataset.org/#keypoints-leaderboard'>COCO Keypoint</a> and <a href='http://placeschallenge.csail.mit.edu/results_challenge.html'>Places Instance Segmentation</a>, as well as the 2nd place of <a href='http://cocodataset.org/#detections-leaderboard'>COCO Instance Segmentation</a>. I was invited to present at COCO & Places Joint Workshop at <a href='http://iccv2017.thecvf.com'>ICCV17</a> in Venice, Italy. </li>
                <div>
            </div></div></ul>
    </td></tr></tbody>
</table>

<!-- publication -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr></tbody>
</table>

<!-- xiao2021early -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr>
    <td width="33%" valign="center" align="center">
        <img src="images/vitconv.png" style="display: inline" width="100%">
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2106.14881" id="xiao2021early">
                <heading>Early Convolutions Help Transformers See Better</heading>
            </a><br>
            <b>Tete Xiao</b>, 
            <a href='https://scholar.google.com/citations?user=QOO8OCcAAAAJ&hl=en'>Mannat Singh</a>, 
            <a href='https://scholar.google.ca/citations?user=T94KevkAAAAJ&hl=en'>Eric Mintun</a>, 
            <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell</a>, 
            <a href='https://pdollar.github.io/'>Piotr Dollár*</a>,<br>
            <a href='https://www.rossgirshick.info/'>Ross Girshick*</a><br> 
            Tech report, 2021 <br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/abs/2106.14881">arXiv</a> |
        </p>
        <p>
            We analyze the substandard optimization behavior of ViT and propose a simple fix that dramatically increases optimization stability and also improves peak performance.
        </p>
    </td>

</tr>
</tbody></table>

<!-- xiao2021region -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="resim_stop()" onmouseover="resim_start()">
    <td width="33%" valign="center" align="center">
        <div class="image_mouseout">
            <div class="image_mouseover" id="resim_image" style="opacity: 0;">
                <img src="images/resim_after.png" ,="" width="100%">
            </div>
            <div class="image_constant" id="resim_constant" style="opacity: 1;">
                <img src="images/resim_before.png" ,="" width="100%">
            </div>
        </div>
        <script type="text/javascript">
            function resim_start() {
                document.getElementById('resim_constant').style.opacity = "0";
                document.getElementById('resim_image').style.opacity = "1";
            }

            function resim_stop() {
                document.getElementById('resim_image').style.opacity = "0";
                document.getElementById('resim_constant').style.opacity = "1";
            }
            resim_stop()
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2103.12902" id="xiao2021region">
                <heading>Region Similarity Representation Learning</heading>
            </a><br>
            <b>Tete Xiao*</b>, 
            <a href='http://people.eecs.berkeley.edu/~cjrd/'>Colorado Reed*</a>, 
            <a href='https://xiaolonw.github.io/'>Xiaolong Wang</a>, 
            <a href='https://people.eecs.berkeley.edu/~keutzer/'>Kurt Keutzer</a>, 
            <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell</a><br> 
            International Conference on Computer Vision (<b>ICCV</b>), 2021<br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/abs/2103.12902">arXiv</a> |
            <a href="https://github.com/Tete-Xiao/ReSim">code</a> |
        </p>
        <p>
            An approach to self-supervised representation learning for localization-based tasks such as object detection and segmentation.
        </p>
    </td>

</tr>
</tbody></table>

<!-- xiao2020should -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="ssl_stop()" onmouseover="ssl_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="ssl_input" class="hidden" style="display: inline;">
            <img src="images/ssl_before.png" width="100%">
        </div>
        <div id="ssl_animate" style="display: none;">
            <a href="images/ssl_after.gif">
                <img src="images/ssl_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function ssl_start() {
                document.getElementById("ssl_animate").style.display = "inline";
                document.getElementById("ssl_input").style.display = "none";
            }

            function ssl_stop() {
                document.getElementById("ssl_animate").style.display = "none";
                document.getElementById("ssl_input").style.display = "inline";
            }
            ssl_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2008.05659" id="xiao2020should">
                <heading>What Should Not Be Contrastive in Contrastive Learning</heading>
            </a><br>
            <b>Tete Xiao</b>, 
            <a href='https://xiaolonw.github.io/'>Xiaolong Wang</a>, 
            <a href='http://people.eecs.berkeley.edu/~efros/'>Alexei A. Efros</a>, 
            <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell</a><br> 
            International Conference on Learning Representations (<b>ICLR</b>), 2021<br>
          | <a href="https://arxiv.org/abs/2008.05659">arXiv</a> |
            <a href="https://www.youtube.com/watch?v=RajmUQKLG2U">video</a> |
        </p>
        <p>
            To contrast, or not to contrast, that is the question.
        </p>
    </td>

</tr>
</tbody></table>

<!-- zhang2020learning -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="cycledynamics_stop()" onmouseover="cycledynamics_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="cycledynamics_input" class="hidden" style="display: inline;">
            <img src="images/cycledynamics_before.png" width="100%">
        </div>
        <div id="cycledynamics_animate" style="display: none;">
            <a href="images/cycledynamics_after.gif">
                <img src="images/cycledynamics_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function cycledynamics_start() {
                document.getElementById("cycledynamics_animate").style.display = "inline";
                document.getElementById("cycledynamics_input").style.display = "none";
            }

            function cycledynamics_stop() {
                document.getElementById("cycledynamics_animate").style.display = "none";
                document.getElementById("cycledynamics_input").style.display = "inline";
            }
            cycledynamics_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://sjtuzq.github.io/cycle_dynamics.html" id="zhang2020learning">
                <heading>Learning Cross-domain Correspondence for Control with Dynamics Cycle-consistency</heading>
            </a><br>
            <a href='https://scholar.google.com/citations?user=mapNJjcAAAAJ&hl=en'>Qiang Zhang</a>, 
            <b>Tete Xiao</b>, 
            <a href='http://people.eecs.berkeley.edu/~efros/'>Alexei A. Efros</a>, 
            <a href='https://cs.nyu.edu/~lp91/'>Lerrel Pinto</a>, 
            <a href='https://xiaolonw.github.io/'>Xiaolong Wang</a><br> 
            International Conference on Learning Representations (<b>ICLR</b>), 2021<br>
            <b>Oral presentation</b><br>
          | <a href="https://sjtuzq.github.io/cycle_dynamics.html">project page</a> |
            <a href="https://arxiv.org/abs/2012.09811">arXiv</a> |
            <a href="https://www.youtube.com/watch?v=CZAsUM96oVg">video</a> |
        </p>
        <p>
            Learning correspondence across domains differing in representation (vision vs. internal state), physics parameters, and morphology.
        </p>
    </td>

</tr>
</tbody></table>

<!-- materzynska2020something -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="sthelse_stop()" onmouseover="sthelse_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="sthelse_input" class="hidden" style="display: inline;">
            <img src="images/sthelse_before.png" width="100%">
        </div>
        <div id="sthelse_animate" style="display: none;">
            <a href="images/sthelse_after.gif">
                <img src="images/sthelse_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function sthelse_start() {
                document.getElementById("sthelse_animate").style.display = "inline";
                document.getElementById("sthelse_input").style.display = "none";
            }

            function sthelse_stop() {
                document.getElementById("sthelse_animate").style.display = "none";
                document.getElementById("sthelse_input").style.display = "inline";
            }
            sthelse_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://joaanna.github.io/something_else/" id="materzynska2020something">
                <heading>Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks</heading>
            </a><br>
            <a href='https://joaanna.github.io/'>Joanna Materzynska</a>, 
            <b>Tete Xiao</b>, 
            <a href='https://roeiherz.github.io/'>Roei Herzig</a>, 
            <a href='https://cs-people.bu.edu/hxu/'>Huijuan Xu<sup>&#8224</sup></a>, 
            <a href='https://xiaolonw.github.io/'>Xiaolong Wang<sup>&#8224</sup></a>, 
            <a href='https://people.eecs.berkeley.edu/~trevor/'>Trevor Darrell<sup>&#8224</sup></a><br> 
            <sup>&#8224</sup>: equal advising<br>
            Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020<br>
          | <a href="https://joaanna.github.io/something_else/">project page</a> |
            <a href="https://arxiv.org/abs/1912.09930">arXiv</a> |
            <a href="https://github.com/joaanna/something_else">dataset</a> |
        </p>
        <p>
            Using Spatial-Temporal Interaction Networks (STIN) for compositional action recognition plus a new annotated dataset Something-else.
        </p>
    </td>

</tr>
</tbody></table>

<!-- xiao2019reasoning -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="dualattention_stop()" onmouseover="dualattention_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="dualattention_input" class="hidden" style="display: inline;">
            <img src="images/dualattention_before.png" width="100%">
        </div>
        <div id="dualattention_animate" style="display: none;">
            <a href="images/dualattention_after.gif">
                <img src="images/dualattention_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function dualattention_start() {
                document.getElementById("dualattention_animate").style.display = "inline";
                document.getElementById("dualattention_input").style.display = "none";
            }

            function dualattention_stop() {
                document.getElementById("dualattention_animate").style.display = "none";
                document.getElementById("dualattention_input").style.display = "inline";
            }
            dualattention_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://dual-attention-network.github.io/" id="xiao2019reasoning">
                <heading>Reasoning About Human-Object Interactions Through Dual Attention Networks</heading>
            </a><br>
            <b>Tete Xiao</b>, 
            <a href='https://researcher.watson.ibm.com/researcher/view.php?person=us-qfan'>Quanfu Fan</a>, 
            <a href='https://researcher.watson.ibm.com/researcher/view.php?person=us-dgutfre'>Dan Gutfreund</a>, 
            <a href='http://people.csail.mit.edu/mmonfort/'>Mathew Monfort</a>, 
            <a href='http://olivalab.mit.edu/audeoliva.html'>Aude Oliva</a>,
            <a href='http://bzhou.ie.cuhk.edu.hk/'>Bolei Zhou</a><br> 
            International Conference on Computer Vision (<b>ICCV</b>), 2019<br>
          | <a href="https://dual-attention-network.github.io/">project page</a> |
            <a href="https://arxiv.org/pdf/1909.04743.pdf">arXiv</a> |
        </p>
        <p>
            Dual Attention Network model reasoning about human-object interactions.
        </p>
    </td>

</tr>
</tbody></table>

<!-- zhou2019semantic -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="ade20k_stop()" onmouseover="ade20k_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="ade20k_input" class="hidden" style="display: inline;">
            <img src="images/ade20k_before.jpg" width="100%">
        </div>
        <div id="ade20k_animate" style="display: none;">
            <a href="images/ade20k_after.gif">
                <img src="images/ade20k_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function ade20k_start() {
                document.getElementById("ade20k_animate").style.display = "inline";
                document.getElementById("ade20k_input").style.display = "none";
            }

            function ade20k_stop() {
                document.getElementById("ade20k_animate").style.display = "none";
                document.getElementById("ade20k_input").style.display = "inline";
            }
            ade20k_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://link.springer.com/article/10.1007/s11263-018-1140-0" id="zhou2019semantic">
                <heading>Semantic Understanding of Scenes through the ADE20K Dataset</heading>
            </a><br>
            <a href='http://bzhou.ie.cuhk.edu.hk/'>Bolei Zhou</a>, 
            <a href='http://www.mit.edu/~hangzhao/'>Hang Zhao</a>, 
            <a href='https://people.csail.mit.edu/xavierpuig/'>Xavier Puig</a>, 
            <b>Tete Xiao</b>, 
            <a href='https://www.cs.utoronto.ca/~fidler/'>Sanja Fidler</a>, 
            <a href='https://groups.csail.mit.edu/vision/torralbalab/'>Adela Barriuso</a>, 
            <a href='https://groups.csail.mit.edu/vision/torralbalab/'>Antonio Torralba</a><br> 
            International Journal of Computer Vision (<b>IJCV</b>) 127, 302–321 (2019)<br>
          | <a href="http://sceneparsing.csail.mit.edu/">project page</a> |
            <a href="pdfs/zhou2019semantic.pdf">pdf</a> |
            <a href="https://arxiv.org/abs/1608.05442">arXiv</a> |
            <a href="https://github.com/CSAILVision/semantic-segmentation-pytorch">pytorch model</a> |
            <a href="http://scenesegmentation.csail.mit.edu/">demo</a> |
        </p>
        <p>
            ADE20K dataset with comprehensive analysis and applications.
        </p>
    </td>

</tr>
</tbody></table>

<!-- xiao2018unified -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="upernet_stop()" onmouseover="upernet_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="upernet_input" class="hidden" style="display: inline;">
            <img src="images/upernet_input_raw.png" width="100%">
        </div>
        <div id="upernet_parsing" style="display: none;">
            <a href="images/upernet_parsing.gif">
                <img src="images/upernet_parsing.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function upernet_start() {
                document.getElementById("upernet_parsing").style.display = "inline";
                document.getElementById("upernet_input").style.display = "none";
            }

            function upernet_stop() {
                document.getElementById("upernet_parsing").style.display = "none";
                document.getElementById("upernet_input").style.display = "inline";
            }
            upernet_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1807.10221" id="xiao2018unified">
                <heading>Unified Perceptual Parsing for Scene Understanding</heading>
            </a><br>
            <b>Tete Xiao*</b>, 
            <a href='http://yingchengliu.com/'>Yingcheng Liu*</a>, 
            <a href='http://bzhou.ie.cuhk.edu.hk/'>Bolei Zhou*</a>, 
            <a href='https://yuningjiang.github.io/'>Yuning Jiang</a>,
            <a href='http://www.jiansun.org/'>Jian Sun</a><br> 
            European Conference on Computer Vision (<b>ECCV</b>), 2018<br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/pdf/1807.10221.pdf">arXiv</a> |
            <a href="https://github.com/CSAILVision/unifiedparsing">code</a> |
        </p>
        <p>
            Pyramid-like parser UPerNet used for Unified Perceptual Parsing task to recognize as many visual concepts as possible from a given image.
        </p>
    </td>

</tr>
</tbody></table>


<!-- jiang2018acquisition -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="iounet_stop()" onmouseover="iounet_start()">
    <td width="33%" valign="center" align="center">
        <div class="image_mouseout">
            <div class="image_mouseover" id="iounet_image" style="opacity: 0;">
                <img src="images/iounet_after.jpg" ,="" width="100%">
            </div>
            <img src="images/iounet_before.jpg" ,="" width="100%">
        </div>
        <script type="text/javascript">
            function iounet_start() {
                document.getElementById('iounet_image').style.opacity = "1";
            }

            function iounet_stop() {
                document.getElementById('iounet_image').style.opacity = "0";
            }
            iounet_stop()
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1807.11590" id="jiang2018acquisition">
                <heading>Acquisition of Localization Confidence for Accurate Object Detection</heading>
            </a><br>
            <a href='https://dblp.org/pid/224/0327.html'>Borui Jiang*</a>, 
            <a href='https://luoruixuan.github.io/'>Ruixuan Luo*</a>, 
            <a href='https://jiayuanm.com/'>Jiayuan Mao*</a>, 
            <b>Tete Xiao</b>, 
            <a href='https://yuningjiang.github.io/'>Yuning Jiang</a><br> 
            European Conference on Computer Vision (<b>ECCV</b>), 2018<br>
            <b>Oral presentation</b><br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/pdf/1807.11590.pdf">arXiv</a> |
            <a href="https://github.com/vacancy/PreciseRoIPooling">code</a> |
        </p>
        <p>
            Dissecting object localization through IouNet and Precise RoI Pooling.
        </p>
    </td>

</tr>
</tbody></table>

<!-- shi2018learning -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="vlcontrastive_stop()" onmouseover="vlcontrastive_start()">
    
    <td width="33%" valign="center" align="center">
        <div id="vlcontrastive_input" class="hidden" style="display: inline;">
            <img src="images/vlcontrastive_before.png" width="100%">
        </div>
        <div id="vlcontrastive_animate" style="display: none;">
            <a href="images/vlcontrastive_after.gif">
                <img src="images/vlcontrastive_after.gif" width="100%">
            </a>
        </div>
        <script charset="utf-8" type="text/javascript">
            function vlcontrastive_start() {
                document.getElementById("vlcontrastive_animate").style.display = "inline";
                document.getElementById("vlcontrastive_input").style.display = "none";
            }

            function vlcontrastive_stop() {
                document.getElementById("vlcontrastive_animate").style.display = "none";
                document.getElementById("vlcontrastive_input").style.display = "inline";
            }
            vlcontrastive_stop() 
        </script>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1806.10348" id="shi2018learning">
                <heading>Learning Visually-grounded Semantics from Contrastive Adversarial Samples</heading>
            </a><br>
            <a href='https://ttic.uchicago.edu/~freda/'>Haoyue Shi*</a>,
            <a href='https://jiayuanm.com/'>Jiayuan Mao*</a>, 
            <b>Tete Xiao*</b>, 
            <a href='https://yuningjiang.github.io/'>Yuning Jiang</a>,
            <a href='http://www.jiansun.org/'>Jian Sun</a><br> 
            International Conference on Computational Linguistics (<b>COLING</b>), 2018<br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/abs/1806.10348">arXiv</a> |
            <a href="https://github.com/ExplorerFreda/VSE-C">code</a> |
        </p>
        <p>
            Constructing constrastive image-caption pairs for learning visually-grounded semantics.
        </p>
    </td>

</tr>
</tbody></table>

<!-- peng2018megdet -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr>
    <td width="33%" valign="center" align="center">
        <img src="images/megdet.png" style="display: inline" width="100%">
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1711.07240" id="peng2018megdet">
                <heading>MegDet: A Large Mini-Batch Object Detector</heading>
            </a><br>
            Chao Peng*, 
            <b>Tete Xiao*</b>, 
            Zeming Li*, 
            Yuning Jiang, 
            Xiangyu Zhang, 
            Kai Jia, 
            Gang Yu, 
            Jian Sun<br> 
            Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018 (Spotlight)<br>
            *: equal contribution<br>
          | <a href="https://arxiv.org/abs/1711.07240">arXiv</a> |
        </p>
        <p>
            Scaling-up training of object detectors; winner of MSCOCO Challenge 2017.
        </p>
    </td>

</tr>
</tbody></table>

<!-- wang2018repulsion -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr>
    <td width="33%" valign="center" align="center">
        <img src="images/reploss.png" style="display: inline" width="100%">
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1711.07752" id="wang2018repulsion">
                <heading>Repulsion Loss: Detecting Pedestrians in a Crowd</heading>
            </a><br>
            Xinlong Wang, 
            <b>Tete Xiao</b>, 
            Yuning Jiang, 
            Shuai Shao, 
            Jian Sun, 
            Chunhua Shen<br> 
            Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018<br>
          | <a href="https://arxiv.org/abs/1711.07752">arXiv</a> |
        </p>
        <p>
            The pedestrian detector that works better for crowd occlusion.
        </p>
    </td>

</tr>
</tbody></table>

<!-- mao2017can -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr>
    <td width="33%" valign="center" align="center">
        <img src="images/whatcanhelppd.png" style="display: inline" width="100%">
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/1705.02757" id="mao2017can">
                <heading>What Can Help Pedestrian Detection?</heading>
            </a><br>
            Jiayuan Mao*, 
            <b>Tete Xiao*</b>, 
            Yuning Jiang, 
            Zhimin Cao<br> 
            Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017<br>
          | <a href="https://arxiv.org/abs/1705.02757">arXiv</a> |
        </p>
    </td>

</tr>
</tbody></table>

<!-- award -->

<table width="100%" align="center" border="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Awards</sectionheading>
    <ul>
        <li><b>MSCOCO Challenge</b>, 2017</li>
        <li><b>Snap Research Scholarship</b>, 2019</li>
        <li><b>China National Scholarship</b>, Peking Univsity</li>
        <li><b>Scholarship for the Outstanding Talented</b>, Peking Univsity</li>
        <li><b>Schlumberger Scholarship</b>, Peking Univsity</li>
        <li><b>Founder Group Scholarship</b>, Peking Univsity</li>
        <li><b>Gold Medals</b>, ACM International Collegiate Programming Contest (ACM-ICPC) Asia Regional, 2016 & 2017</li>
        <li><b>Bronze Medal</b>, National Olympiad in Informatics (NOI), 2014</li>
        <li><b>Champion</b>, Shandong Province Team Selection Contest for NOI, 2014</li>
    </ul>
  </td></tr></tbody>
</table>

<!-- service -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Service</sectionheading>
  <tr>
    <td width="15%" valign="center" align="center"><img style="display: inline" width="100%" src="images/pku_seal.png"></td>
    <td width="85%" valign="center">
      Teaching Faculty, Practice in Programming (17-18 spring)
      <br><br>
      Teaching Faculty, Artificial Intelligence and Computer Vision (18-19 spring)</a>
    </td>
  </tr>
</tbody></table>

<!-- contact -->

<table width="100%" align="center" border="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Contact</sectionheading>
        <p style="margin-left: 5%;">
        Berkeley Artificial Intelligence Research Lab<br> 
        Berkeley Way West, 2121 Berkeley Way<br>
        Berkeley, CA 94704
        </p>
  </td></tr></tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody><tr><td><br>
    <p align="right"><font size="2">
        <b>Website design</b>: <a href="http://www.cs.berkeley.edu/~barron/">✩</a> <a href="https://people.eecs.berkeley.edu/~pathak/">✩</a><br>
        <b>Avatar photo</b>: taken in Jerusalem in July 2019 by my good friend <a href="http://yingchengliu.com/">Yingcheng Liu</a>.
    </font></p>
</td></tr></tbody></table>
        
</td></tr></tbody></table>

</body></html>
